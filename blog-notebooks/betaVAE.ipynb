{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\beta - \\text{VAE}$ Framework Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kullback-Leibler (KL) divergence**<br><br>\n",
    "Kullback-Leibler (KL) divergence, a measure of the difference between two probability distributions over the same variable ($x$). For two probability distributions $p(x)$ and $q(x)$, the KL divergence of $q(x)$ from $p(x)$, denoted as $D_{\\text{KL}}(p(x) || q(x))$, is defined as:\n",
    "\n",
    "- If $x$ is continuous random variable:<br>\n",
    "$\\begin{equation}D_{\\text{KL}}(p(x) || q(x)) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\\end{equation}$\n",
    "- If $x$ is discrete random variable:<br>\n",
    "$\\begin{equation}D_{\\text{KL}}(p(x) || q(x)) = \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)}\\end{equation}$\n",
    "\n",
    "Notice that $D_{\\text{KL}}(p(x) || q(x))$ is a measure of the information lost when using $q(x)$ to approximate $p(x)$<sup>[1]</sup>, not a true distance metric (unlike Euclidean distance or total variation distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational Inference**<br><br>\n",
    "Variational Inference (VI) is a method for approximating the posterior distribution of latent variables (denoted as $z$) given observed data (denoted as $x$).<br>\n",
    "\n",
    "We can always write the conditional distribution of $z$ given $x$ as:\n",
    "\n",
    "$$\n",
    "p(z|x) = \\frac{p(z, x)}{p(x)}\n",
    "$$\n",
    "\n",
    "here $p(x)$ is the marginal distribution of the observed data (a.k.a. evidence), calculated as $p(x) = \\int p(z, x) dz$. However, this integral is often intractable, and we need to use VI to approximate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
