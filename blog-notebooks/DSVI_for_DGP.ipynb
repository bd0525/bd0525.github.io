{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Gaussian Process (DGP), a hierarchical composition of Gaussian Processes(GP), can overcome the limitations of standard (single-layer) GP while maintaining the benefits of GP<sup>[1]</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\require{cancel}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (Single-layer) Gaussian Processes  \n",
    "\n",
    ">Consider inferring a stochastic function $f:\\mathbb{R}^{D}\\to\\mathbb{R}$ , given a likelihood $p(y|f)$ and a set of $N$ observations $\\mathbf{y}=(y_{1},\\dots,y_{N})^{\\top}$ at (design) locations $\\mathbf X=(\\mathbf x_{1},\\dots,\\mathbf x_{N})^{\\top}$. \n",
    "\n",
    "> Place a GP prior on function $f$ so that all function values as jointly Gaussian, with a mean function $m:\\mathbb{R}^{D}\\rightarrow\\mathbf{R}$ and a covariance function $k:\\mathbb{R}^{D}\\times\\mathbb{R}^{D}\\overset{.}{\\to}\\mathbb{R}$.\n",
    "\n",
    ">Define an additional set of $M$ inducing locations $\\mathbf{Z}=(\\mathbf{z}_{1}, \\cdots,\\mathbf{z}_{M})^{\\top}$. Use the notation $\\mathbf{f}=f(\\mathbf{X})$ and $\\mathbf{u}=f(\\mathbf{Z})$ to represent the function values at the design and inducing locations.\n",
    "\n",
    "By the definition of a GP, the joint density $p(\\mathbf{f},\\mathbf{u})$ is a Gaussian whose mean is given by the mean function evaluated at every input $(\\mathbf{X},\\mathbf{Z})^{\\top}$ , and the corresponding covariance is given by the covariance function evaluated at every pair of inputs.\n",
    "$$\n",
    "\\begin{bmatrix} \\mathbf{f} \\\\ \\mathbf{u} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} m(\\mathbf{X}) \\\\ m(\\mathbf{Z}) \\end{bmatrix}, \\begin{bmatrix} K_{\\mathbf{XX}} & K_{\\mathbf{XZ}} \\\\ K_{\\mathbf{ZX}} & K_{\\mathbf{ZZ}} \\end{bmatrix} \\right) \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "The joint density of $\\mathbf{y},\\mathbf{f}$ and $\\mathbf{u}$ is given by:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{y},\\mathbf{f},\\mathbf{u})&= p(\\mathbf{f}, \\mathbf{u}) p(\\mathbf{y}|\\mathbf{f}, \\cancel{\\mathbf{u}}) \\\\\n",
    "&= p(\\mathbf{f}|\\mathbf{u}) p(\\mathbf{u}) \\prod_{i=1}^{N}p(y_{i}|f_{i}) \\\\\n",
    "&=\\underbrace{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})p(\\mathbf{u};\\mathbf{Z})}_{\\mathrm{GP~prior}}\\underbrace{\\prod_{i=1}^{N}p(y_{i}|f_{i})}_{\\mathrm{likelihood}}\n",
    "\\end{aligned} \\tag{2}\n",
    "$$  \n",
    "\n",
    "**Notice** that $p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})$ indicates that the input locations for $\\mathbf{f}$ and $\\mathbf{u}$ are $\\mathbf{X}$ and $\\mathbf{Z}$, respectively.\n",
    "\n",
    "The prior $p(\\mathbf{u}; \\mathbf{Z}) = \\mathcal{N}(\\mathbf{u} | m(\\mathbf{Z}), k(\\mathbf{Z}, \\mathbf{Z}))$ and the conditional $p(\\mathbf{f} | \\mathbf{u}; \\mathbf{X}, \\mathbf{Z}) = \\mathcal{N}(\\mathbf{f} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})$, where for $i, j = 1, \\ldots, N$:\n",
    "\n",
    "$$[\\boldsymbol{\\mu}]_i = m(\\mathbf{x}_i) + \\boldsymbol{\\alpha}(\\mathbf{x}_i)^{\\top} (\\mathbf{u} - m(\\mathbf{Z})) \\tag{3}$$\n",
    "$$[\\mathbf{\\Sigma}]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) - \\boldsymbol{\\alpha}(\\mathbf{x}_i)^{\\top} k(\\mathbf{Z}, \\mathbf{Z}) \\boldsymbol{\\alpha}(\\mathbf{x}_j) \\tag{4}$$\n",
    "\n",
    "with $\\boldsymbol{\\alpha}(\\mathbf{x}_{i})=k(\\mathbf{Z},\\mathbf{Z})^{-1}k(\\mathbf{Z},\\mathbf{x}_{i})$. Inference is possible in closed form when the likelihood $p(y|f)$ is Gaussian, but the time complexity is $O(N^3)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was originally proposed to address challenges associated with large datasets and non-Gaussian likelihoods. To tackle both of these issues simultaneously, the authors seek a variational posterior. Variational inference aims to approximate the true posterior distribution $p(\\mathbf{f},\\mathbf{u})$ with a simpler distribution $q(\\mathbf{f},\\mathbf{u})$, by minimizing the Kullback-Leibler divergence $\\operatorname{KL}[q||p]$ between the variational posterior $q$ and the true posterior $p$ . Equivalently, this can be achieved by maximizing the a lower bound on the marginal likelihood (a.k.a. evidence):  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right] \\tag{5}\n",
    "$$  \n",
    "\n",
    "where $p(\\mathbf{y},\\mathbf{f},\\mathbf{u})$ is given in (2). The authors then choose a variational posterior based on the work of Hensman et al. (2013), **_Gaussian process for Big Data_**:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u}) \\tag{6}\n",
    "$$  \n",
    "\n",
    "where $q(\\mathbf{u})=\\mathcal{N}(\\mathbf{u}|\\mathbf{m},\\mathbf{S})$ . Since both terms in the variational posterior are Gaussian, marginalizing $\\mathbf{u}$ yields:  \n",
    "\n",
    "$$\n",
    "q(\\mathbf{f}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=\\int p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u})d\\mathbf{u}=\\mathcal{N}(\\mathbf{f}|\\tilde{\\boldsymbol{\\mu}},\\tilde{\\boldsymbol{\\Sigma}}) \\tag{7}\n",
    "$$  \n",
    "\n",
    "$\\tilde{\\boldsymbol{\\mu}}$ and $\\tilde{\\boldsymbol{\\Sigma}}$ can be expressed as the mean and covariance functions of the inputs, similar to equations (3) and (4). This form is commonly used in Bayesian inference with Gaussian random fields (GRF):\n",
    "\n",
    "$$\n",
    "\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i})=m(\\mathbf{x}_{i})+\\boldsymbol{\\alpha}(\\mathbf{x}_{i})^{\\top}(\\mathbf{m}-m(\\mathbf{Z})) \\tag{8}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{j})=k(\\mathbf{x}_{i},\\mathbf{x}_{j})-\\boldsymbol{\\alpha}(\\mathbf{x}_{i})^{\\top}(k(\\mathbf{Z},\\mathbf{Z})-\\mathbf{S})\\boldsymbol{\\alpha}(\\mathbf{x}_{j}) \\tag{9}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "The $f_{i}$ marginals of the variational posterior (7) depend only on the corresponding inputs $\\mathbf{x}_{i}$ . Therefore, the i<sup>th</sup> marginal of $q(\\mathbf{f}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})$ can be written as:  \n",
    "\n",
    "$$\n",
    "q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})=\\mathcal{N}(f_{i}|\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i}),\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{i})) \\tag{10}\n",
    "$$ \n",
    "<pre> <code>\n",
    " Observations:      y1         y*         yc\n",
    "                    |          |          |\n",
    "Gaussian field:     f1 -- o -- f* -- o -- fc\n",
    "                    |     |    |     |    |\n",
    "     Inputs:        x1    x2   x*   ...   xc\n",
    "</code> </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the variational posterior (6) the lower bound (5) simplifies considerably since:\n",
    "- The conditionals $p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})$ inside the logarithm cancel\n",
    "- The likelihood expectation requires only the variational marginals\n",
    "\n",
    "Let's start with the evidence lower bound (ELBO) from (5):\n",
    "$$\\begin{aligned}\\mathcal{L}&=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right]\\end{aligned}$$\n",
    "From (6), we have:\n",
    "$$q(\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u})$$ \n",
    "From (2), we know that:\n",
    "$$p(\\mathbf{y},\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})$$\n",
    "So the ratio inside the logarithm is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{p(\\mathbf{y}, \\mathbf{f}, \\mathbf{u})}{q(\\mathbf{f}, \\mathbf{u})} &= \\frac{\\cancel{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})}p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})}{\\cancel{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})}q(\\mathbf{u})} \\\\\n",
    "&= \\frac{p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})}{q(\\mathbf{u})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting back into the ELBO, we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right] \\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})}{q(\\mathbf{u})}\\right] \\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{u})}{q(\\mathbf{u})}\\right] + \\mathbb{E}_{q(\\mathbf{f})}\\left[\\log\\prod_{i=1}^{N}p(y_{i}|f_{i})\\right] \\\\\n",
    "&= - \\mathrm{KL}[q(\\mathbf{u})||p(\\mathbf{u})] + \\sum_{i=1}^{N}\\mathbb{E}_{q(\\mathbf{f_i})}\\left[\\log p(y_{i}|f_{i})\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since the expectation of the likelihood only requires the marginal variational distributions for each $f_{i}$. From (10) we know that the marginals are given by:\n",
    "$$q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})=\\mathcal{N}(f_{i}|\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i}),\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{i}))$$\n",
    "\n",
    "Thus we arrive at the simplified ELBO:\n",
    "$$\\mathcal{L}=\\sum_{i=1}^{N}\\mathbb{E}_{q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})}[\\log p(y_{i}|f_{i})]-\\mathrm{KL}[q(\\mathbf{u})||p(\\mathbf{u})] \\tag{11}$$\n",
    " \n",
    "\n",
    "The final (univariate) expectation of the log-likelihood can be computed analytically in some cases:\n",
    "- Gauss-Hermite quadrature, Hensman et al., 2015, **_Scalable Variational Gaussian Process Classification_**;\n",
    "- Monte Carlo sampling, Bonilla et al., 2016 **_Generic Inference in Latent Gaussian Process Models_** and Gal et al., 2015 **_Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data_**. \n",
    "\n",
    "The bound is a sum over the data, an unbiased estimator can be obtained through minibatch subsampling. This makes inference feasible on large datasets. A GP uses this inference method referred as a **sparse Gaussian Process (SGP)**.\n",
    "\n",
    "The variational parameters $(\\mathbf{Z},\\mathbf{m}$ and $\\mathbf{S})$ are optimized by maximizing the lower bound we just derived (11). This maximization is guaranteed to converge, as $\\mathcal{L}$ is a valid lower bound to the marginal likelihood $p(\\mathbf{y}|\\mathbf{X})$. The model parameters (hyperparameters of the kernel and likelihood) can also be learned through the maximization this bound. However, caution is needed, as this approach may introduce bias—the bound is not uniformly tight across all hyperparameter settings.\n",
    "\n",
    "For $D$-dimensional outputs $\\mathbf{y}_{i}\\in\\mathbb{R}^{D}$, define $\\mathbf{Y}$ as the matrix with $i$<sup>th</sup> row containing the $i$<sup>th</sup> observation $\\mathbf{y}_{i}$. Similarly, define $\\mathbf{F}$ and $\\mathbf{U}$. If each output is an independent GP we have the GP prior $\\begin{array}{r}{\\prod_{d=1}^{D}p(\\mathbf{F}_{d}|\\mathbf{U}_{d};\\mathbf{X},\\mathbf{Z})p(\\mathbf{U}_{d};\\mathbf{Z})}\\end{array}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference<br>\n",
    "<!-- apa -->\n",
    "[1] Salimbeni, H., & Deisenroth, M. (2017). Doubly stochastic variational inference for deep Gaussian processes. Advances in neural information processing systems, 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
