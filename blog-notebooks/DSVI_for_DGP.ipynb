{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Gaussian Process (DGP), a hierarchical composition of Gaussian Processes(GP), can overcome the limitations of standard (single-layer) GP while maintaining the benefits of GP<sup>[1]</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (Single-layer) Gaussian Processes  \n",
    "\n",
    ">Consider inferring a stochastic function $f:\\mathbb{R}^{D}\\to\\mathbb{R}$ , given a likelihood $p(y|f)$ and a set of $N$ observations $\\mathbf{y}=(y_{1},\\dots,y_{N})^{\\top}$ at (design) locations $\\mathbf X=(\\mathbf x_{1},\\dots,\\mathbf x_{N})^{\\top}$. \n",
    "\n",
    "> Place a GP prior on function $f$ so that all function values as jointly Gaussian, with a mean function $m:\\mathbb{R}^{D}\\rightarrow\\mathbf{R}$ and a covariance function $k:\\mathbb{R}^{D}\\times\\mathbb{R}^{D}\\overset{.}{\\to}\\mathbb{R}$.\n",
    "\n",
    ">Define an additional set of $M$ inducing locations $\\mathbf{Z}=(\\mathbf{z}_{1}, \\cdots,\\mathbf{z}_{M})^{\\top}$. Use the notation $\\mathbf{f}=f(\\mathbf{X})$ and $\\mathbf{u}=f(\\mathbf{Z})$ to represent the function values at the design and inducing locations.\n",
    "\n",
    "By the definition of a GP, the joint density $p(\\mathbf{f},\\mathbf{u})$ is a Gaussian whose mean is given by the mean function evaluated at every input $(\\mathbf{X},\\mathbf{Z})^{\\top}$ , and the corresponding covariance is given by the covariance function evaluated at every pair of inputs.\n",
    "$$\n",
    "\\begin{bmatrix} \\mathbf{f} \\\\ \\mathbf{u} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} m(\\mathbf{X}) \\\\ m(\\mathbf{Z}) \\end{bmatrix}, \\begin{bmatrix} K_{\\mathbf{XX}} & K_{\\mathbf{XZ}} \\\\ K_{\\mathbf{ZX}} & K_{\\mathbf{ZZ}} \\end{bmatrix} \\right) \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "The joint density of $\\mathbf{y},\\mathbf{f}$ and $\\mathbf{u}$ is given by:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{y},\\mathbf{f},\\mathbf{u})&= p(\\mathbf{f}, \\mathbf{u}) p(\\mathbf{y}|\\mathbf{f}, \\not{\\mathbf{u}}) \\\\\n",
    "&= p(\\mathbf{f}|\\mathbf{u}) p(\\mathbf{u}) \\prod_{i=1}^{N}p(y_{i}|f_{i}) \\\\\n",
    "&=\\underbrace{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})p(\\mathbf{u};\\mathbf{Z})}_{\\mathrm{GP~prior}}\\underbrace{\\prod_{i=1}^{N}p(y_{i}|f_{i})}_{\\mathrm{likelihood}}\n",
    "\\end{aligned} \\tag{2}\n",
    "$$  \n",
    "\n",
    "**Notice** that $p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})$ indicates that the input locations for $\\mathbf{f}$ and $\\mathbf{u}$ are $\\mathbf{X}$ and $\\mathbf{Z}$, respectively.\n",
    "\n",
    "The prior $p(\\mathbf{u}; \\mathbf{Z}) = \\mathcal{N}(\\mathbf{u} | m(\\mathbf{Z}), k(\\mathbf{Z}, \\mathbf{Z}))$ and the conditional $p(\\mathbf{f} | \\mathbf{u}; \\mathbf{X}, \\mathbf{Z}) = \\mathcal{N}(\\mathbf{f} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})$, where for $i, j = 1, \\ldots, N$:\n",
    "\n",
    "$$[\\boldsymbol{\\mu}]_i = m(\\mathbf{x}_i) + \\boldsymbol{\\alpha}(\\mathbf{x}_i)^{\\top} (\\mathbf{u} - m(\\mathbf{Z})) \\tag{3}$$\n",
    "$$[\\mathbf{\\Sigma}]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) - \\boldsymbol{\\alpha}(\\mathbf{x}_i)^{\\top} k(\\mathbf{Z}, \\mathbf{Z}) \\boldsymbol{\\alpha}(\\mathbf{x}_j) \\tag{4}$$\n",
    "\n",
    "with $\\boldsymbol{\\alpha}(\\mathbf{x}_{i})=k(\\mathbf{Z},\\mathbf{Z})^{-1}k(\\mathbf{Z},\\mathbf{x}_{i})$. Inference is possible in closed form when the likelihood $p(y|f)$ is Gaussian, but the time complexity is $O(N^3)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method was originally proposed to address challenges associated with large datasets and non-Gaussian likelihoods. To tackle both of these issues simultaneously, the authors seek a variational posterior. Variational inference aims to approximate the true posterior distribution $p(\\mathbf{f},\\mathbf{u})$ with a simpler distribution $q(\\mathbf{f},\\mathbf{u})$, by minimizing the Kullback-Leibler divergence $\\operatorname{KL}[q||p]$ between the variational posterior $q$ and the true posterior $p$ . Equivalently, this can be achieved by maximizing the a lower bound on the marginal likelihood (a.k.a. evidence):  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right] \\tag{5}\n",
    "$$  \n",
    "\n",
    "where $p(\\mathbf{y},\\mathbf{f},\\mathbf{u})$ is given in (2). The authors then choose a variational posterior based on the work of Hensman et al. (2013), **_Gaussian process for Big Data_**:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u}) \\tag{6}\n",
    "$$  \n",
    "\n",
    "where $q(\\mathbf{u})=\\mathcal{N}(\\mathbf{u}|\\mathbf{m},\\mathbf{S})$ . Since both terms in the variational posterior are Gaussian, marginalizing $\\mathbf{u}$ yields:  \n",
    "\n",
    "$$\n",
    "q(\\mathbf{f}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=\\int p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u})d\\mathbf{u}=\\mathcal{N}(\\mathbf{f}|\\tilde{\\boldsymbol{\\mu}},\\tilde{\\boldsymbol{\\Sigma}}) \\tag{7}\n",
    "$$  \n",
    "\n",
    "$\\tilde{\\boldsymbol{\\mu}}$ and $\\tilde{\\boldsymbol{\\Sigma}}$ can be expressed as the mean and covariance functions of the inputs, similar to equations (3) and (4). This form is commonly used in Bayesian inference with Gaussian random fields (GRF):\n",
    "\n",
    "$$\n",
    "\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i})=m(\\mathbf{x}_{i})+\\boldsymbol{\\alpha}(\\mathbf{x}_{i})^{\\top}(\\mathbf{m}-m(\\mathbf{Z})) \\tag{8}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{j})=k(\\mathbf{x}_{i},\\mathbf{x}_{j})-\\boldsymbol{\\alpha}(\\mathbf{x}_{i})^{\\top}(k(\\mathbf{Z},\\mathbf{Z})-\\mathbf{S})\\boldsymbol{\\alpha}(\\mathbf{x}_{j}) \\tag{9}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "The $f_{i}$ marginals of the variational posterior (7) depend only on the corresponding inputs $\\mathbf{x}_{i}$ . Therefore, the i<sup>th</sup> marginal of $q(\\mathbf{f}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})$ can be written as:  \n",
    "\n",
    "$$\n",
    "q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})=\\mathcal{N}(f_{i}|\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i}),\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{i})) \\tag{10}\n",
    "$$ \n",
    "<pre> <code>\n",
    " Observations:      y1         y*         yc\n",
    "                    |          |          |\n",
    "Gaussian field:     f1 -- o -- f* -- o -- fc\n",
    "                    |     |    |     |    |\n",
    "     Inputs:        x1    x2   x*   ...   xc\n",
    "</code> </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the variational posterior (6) the lower bound (5) simplifies considerably since:\n",
    "- The conditionals $p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})$ inside the logarithm cancel\n",
    "- The likelihood expectation requires only the variational marginals\n",
    "\n",
    "$$\\begin{aligned}\\mathcal{L}&=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right]\\end{aligned}$$\n",
    "\n",
    "$$q(\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u})$$ \n",
    "\n",
    "$$q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})=\\mathcal{N}(f_{i}|\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i}),\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{i}))$$ \n",
    "\n",
    "$$\\mathcal{L}=\\sum_{i=1}^{N}\\mathbb{E}_{q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})}[\\log p(y_{i}|f_{i})]-\\mathrm{KL}[q(\\mathbf{u})||p(\\mathbf{u})] \\tag{11}$$\n",
    " \n",
    "\n",
    "The final (univariate) expectation of the log-likelihood can be computed analytically in some cases:\n",
    "- Gauss-Hermite quadrature, Hensman et al., 2015, **_Scalable Variational Gaussian Process Classification_**;\n",
    "- Monte Carlo sampling, Bonilla et al., 2016 **_Generic Inference in Latent Gaussian Process Models_** and Gal et al., 2015 **_Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data_**. \n",
    "\n",
    "Since the bound is a sum over the data, an unbiased estimator can be obtained through minibatch subsampling. This permits inference on large datasets. In this work we refer to a GP with this method of inference as a sparse $G P(S G P)$.\n",
    "\n",
    "The variational parameters $(\\mathbf{Z},\\mathbf{m}$ and $\\mathbf{S})$ are found by maximizing the lower bound (11). This maximization is guaranteed to converge since $\\mathcal{L}$ is a lower bound to the marginal likelihood $p(\\mathbf{y}|\\mathbf{X})$.\n",
    "\n",
    "We can also learn model parameters (hyperparameters of the kernel or likelihood) through the maximization of this bound, \n",
    "\n",
    "though we should exercise caution as this introduces bias because the bound is not uniformly tight for all settings of hyperparameters, but in some cases, a bias is acceptable and necessary, share the same idea with $\\beta$-VAE. A $\\beta$ constant can be introduced to reduce the regularization effect of the KL divergence, when to use it?\n",
    "- If your experiment is limited and expensive, and you have a strong belief that your experiment is the thing you can mostly trust;\n",
    "\n",
    "In the case of $D$ -dimensional outputs $\\mathbf{y}_{i}\\in\\mathbb{R}^{D}$ we define $\\mathbf{Y}$ as the matrix with $i$ th row containing the $i$ th observation $\\mathbf{y}_{i}$ . Similarly, we define $\\mathbf{F}$ and $\\mathbf{U}$ If each output is an independent GP we have the GP prior $\\begin{array}{r}{\\prod_{d=1}^{D}p(\\mathbf{F}_{d}|\\mathbf{U}_{d};\\mathbf{X},\\mathbf{Z})p(\\mathbf{U}_{d};\\mathbf{Z})}\\end{array}$ , which we abbreviate as $p(\\mathbf{F}|\\mathbf{U};\\mathbf{\\bar{X}},\\mathbf{Z})p(\\mathbf{U};\\mathbf{Z})$ to lighten the notat ion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference<br>\n",
    "<!-- apa -->\n",
    "[1] Salimbeni, H., & Deisenroth, M. (2017). Doubly stochastic variational inference for deep Gaussian processes. Advances in neural information processing systems, 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
